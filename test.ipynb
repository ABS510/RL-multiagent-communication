{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "import imageio\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 1  # Number of episodes for training\n",
    "BATCH_SIZE = 32  # Batch size for replay buffer sampling\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LEARNING_RATE = 1e-3  # Learning rate for optimizer\n",
    "BUFFER_SIZE = 100  # Size of replay buffer\n",
    "TARGET_UPDATE = 10  # Frequency to update target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network architecture\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3 * 210 * 160, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x.shape = (batch_size, 3, 210, 160)\n",
    "        return self.net(x.reshape(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.q_network = DQN(action_size).to(device)\n",
    "        self.target_network = DQN(action_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(\n",
    "                    state, device=device, dtype=torch.float32\n",
    "                ).unsqueeze(0)\n",
    "                q_values = self.q_network(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < BATCH_SIZE:\n",
    "            return  # Wait until buffer has enough samples\n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        batch = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "        next_states = torch.tensor(\n",
    "            np.array(next_states), device=device, dtype=torch.float32\n",
    "        )\n",
    "        dones = torch.tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Compute target Q-values for next states\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and optimize\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network periodically\n",
    "        if self.steps_done % TARGET_UPDATE == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = space_invaders_v2.env(render_mode=\"rgb_array\")  # Render mode for capturing frames\n",
    "agent = DQNAgent(action_size=env.action_space(env.possible_agents[0]).n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/yfdnq0l94cl3g1s0q8p1m2gc0000gq/T/ipykernel_14864/3893003470.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 55\u001b[0m\n\u001b[1;32m     40\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     41\u001b[0m     (\n\u001b[1;32m     42\u001b[0m         observation\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m agent\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# End the loop if the game is over\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 32\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     31\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(states), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 32\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     34\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     35\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(next_states), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in range(EPISODES):\n",
    "    env.reset(seed=42)\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initialize video writer\n",
    "    frames = []\n",
    "\n",
    "    for agent_id in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "\n",
    "        # Preprocess observation (resize and normalize)\n",
    "        if observation is not None:\n",
    "            observation = (\n",
    "                np.transpose(observation, (2, 0, 1)) / 255.0\n",
    "            )  # Normalize pixel values\n",
    "            observation = torch.tensor(observation, device=device, dtype=torch.float32)\n",
    "\n",
    "        # Select action\n",
    "        action = agent.select_action(observation) if not done else None\n",
    "\n",
    "        # Take action in environment\n",
    "        env.step(action)\n",
    "        next_observation, reward, _, _, _ = env.last()\n",
    "        total_reward += reward\n",
    "\n",
    "        # Capture frame for video\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Process next observation\n",
    "        if next_observation is not None:\n",
    "            next_observation = np.transpose(next_observation, (2, 0, 1)) / 255.0\n",
    "            next_observation = torch.tensor(\n",
    "                next_observation, device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        agent.replay_buffer.add(\n",
    "            (\n",
    "                observation.cpu().numpy(),\n",
    "                action,\n",
    "                reward,\n",
    "                (\n",
    "                    next_observation.cpu().numpy()\n",
    "                    if next_observation is not None\n",
    "                    else None\n",
    "                ),\n",
    "                done,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Train the agent\n",
    "        agent.train()\n",
    "        agent.steps_done += 1\n",
    "\n",
    "        # End the loop if the game is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Save episode as video\n",
    "    video_filename = f\"space_invaders_episode_{episode + 1}.mp4\"\n",
    "    with imageio.get_writer(video_filename, fps=30) as video:\n",
    "        for frame in frames:\n",
    "            video.append_data(frame)\n",
    "    print(\n",
    "        f\"Episode {episode + 1}/{EPISODES}, Total Reward: {total_reward}, Video saved as {video_filename}\"\n",
    "    )\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
